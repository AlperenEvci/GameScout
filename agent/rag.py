#!/usr/bin/env python3
# rag.py - RAG (Retrieval-Augmented Generation) entegrasyonu

import os
import sys
import logging
import queue
import time
from pathlib import Path

# Proje kÃ¶k dizinini ekleyerek diÄŸer modÃ¼lleri iÃ§e aktarabilmemizi saÄŸlayalÄ±m
sys.path.append(str(Path(__file__).parent.parent))

from query import BG3KnowledgeBase
from llm.api_client import LLMAPIClient
from ui.hud_display import HudWindow
from utils.helpers import get_logger

logger = get_logger(__name__)

class RAGAssistant:
    """
    Baldur's Gate 3 oyuncularÄ±na bilgi ve gÃ¶revlerle ilgili yardÄ±m saÄŸlayan RAG sistemi.
    
    Bu sÄ±nÄ±f, vektÃ¶r veritabanÄ± Ã¼zerinde arama yapar, sonuÃ§larÄ± LLM'e prompt olarak gÃ¶nderir
    ve oyuncuya HUD Ã¼zerinde gÃ¶sterir.
    """
      def __init__(self):
        """
        RAG asistanÄ±nÄ± baÅŸlat ve gerekli bileÅŸenleri yÃ¼kle.
        
        Modern RAG mimarisi (2025) ÅŸunlarÄ± iÃ§erir:
        1. GeliÅŸmiÅŸ bilgi tabanÄ± entegrasyonu
        2. Sorgu sonuÃ§larÄ± iÃ§in Ã¶nbellek
        3. Ã‡oklu dil desteÄŸi
        4. Oturum yÃ¶netimi
        5. Metrik takibi
        """
        self.knowledge_base = BG3KnowledgeBase()
        self.llm_client = LLMAPIClient()
        self.hud_queue = queue.Queue()
        self.hud = None
        self.is_initialized = False
        self.last_query_time = 0
        self.rate_limit = 3  # Saniye cinsinden sorgu sÄ±klÄ±ÄŸÄ± limiti
        
        # Ã‡oklu dil desteÄŸi (varsayÄ±lan TÃ¼rkÃ§e)
        self.language = "tr"
        
        # Sorgu Ã¶nbelleÄŸi - sÄ±k sorulan sorular iÃ§in yanÄ±tlarÄ± cache'le
        self.query_cache = {}
        self.max_cache_size = 50  # Maksimum Ã¶nbellek boyutu
        
        # Oturum verisi - kullanÄ±cÄ±nÄ±n sorgu geÃ§miÅŸini tut
        self.session_history = []
        self.max_history = 10
        
        # Performans metrikleri
        self.metrics = {
            "total_queries": 0,
            "successful_queries": 0,
            "failed_queries": 0,
            "cache_hits": 0,
            "avg_response_time": 0,
        }
        
    def initialize(self):
        """Bilgi tabanÄ±nÄ± ve HUD'u baÅŸlat."""
        try:
            # Bilgi tabanÄ±nÄ± yÃ¼kle
            kb_loaded = self.knowledge_base.initialize()
            if not kb_loaded:
                logger.error("Bilgi tabanÄ± yÃ¼klenemedi. Ä°ndeksleme iÅŸlemi Ã§alÄ±ÅŸtÄ±rÄ±ldÄ± mÄ±?")
                return False
            
            # HUD'u baÅŸlat
            self.hud = HudWindow(self.hud_queue)
            self.hud.start()
            self.hud_queue.put("GameScout RAG AsistanÄ± yÃ¼kleniyor...")
            
            self.is_initialized = True
            logger.info("RAG AsistanÄ± baÅŸlatÄ±ldÄ±")
            return True
            
        except Exception as e:
            logger.error(f"RAG AsistanÄ± baÅŸlatÄ±lÄ±rken hata: {str(e)}")
            return False
    
    def rag_search(self, query, top_k=3):
        """Bilgi tabanÄ±nda arama yaparak ilgili belgeleri getir."""
        if not self.is_initialized:
            logger.error("RAG AsistanÄ± baÅŸlatÄ±lmadÄ±. initialize() metodunu Ã§aÄŸÄ±rÄ±n.")
            return []
        
        try:
            logger.info(f"'{query}' iÃ§in bilgi tabanÄ±nda arama yapÄ±lÄ±yor...")
            results = self.knowledge_base.search(query, top_k=top_k)
            logger.info(f"{len(results)} sonuÃ§ bulundu")
            return results
        except Exception as e:
            logger.error(f"Arama sÄ±rasÄ±nda hata: {str(e)}")
            return []
      def _get_context_window(self):
        """API tipine gÃ¶re maksimum baÄŸlam penceresini belirle."""
        # Different LLM providers have different context window limits
        context_limits = {
            "openai": 14000,  # Approximate for gpt-3.5-turbo
            "deepseek": 8000,  # DeepSeek Chat model
            "gemini": 12000,   # Gemini Pro
            # Default to conservative value
            "default": 4000  
        }
        
        from config import settings
        api_type = settings.LLM_API_TYPE.lower()
        return context_limits.get(api_type, context_limits["default"])
        
    def _prepare_contexts(self, contexts, max_length=None):
        """
        Birden fazla baÄŸlamÄ± birleÅŸtirip formatla, token limitlerini aÅŸmamak iÃ§in optimize et.
        
        Yeni stratejiler:
        1. BaÄŸlam penceresine sÄ±ÄŸacak ÅŸekilde iÃ§eriÄŸi kÄ±salt
        2. Ä°Ã§eriÄŸi alakasÄ±na gÃ¶re sÄ±rala
        3. Duplicated iÃ§eriÄŸi temizle
        4. Her bir baÄŸlam kaynaÄŸÄ±nÄ± daha net belirt
        """
        if not max_length:
            max_length = self._get_context_window() - 2000  # Prompt iÃ§in 2000 token rezerve et
        
        formatted_contexts = []
        current_length = 0
        seen_content = set()  # Duplicate iÃ§erik tespiti iÃ§in
        
        # Sort contexts by relevance score if available
        if contexts and "score" in contexts[0]:
            contexts = sorted(contexts, key=lambda x: x.get("score", 0), reverse=True)
        
        for i, context in enumerate(contexts):
            title = context.get('title', 'BaÅŸlÄ±k yok')
            content = context.get('content', 'Ä°Ã§erik yok')
            url = context.get('url', 'URL yok')
            
            # Skip duplicate content
            content_hash = hash(content[:100])  # Use first 100 chars as fingerprint
            if content_hash in seen_content:
                logger.debug(f"Skipping duplicate content: {title}")
                continue
            seen_content.add(content_hash)
            
            # Calculate space needed for this context
            context_header = f"--- Kaynak {i+1}: {title} ---\n"
            context_footer = f"URL: {url}\n---\n"
            
            # Estimate length
            header_length = len(context_header) // 4  # Approximate token count
            footer_length = len(context_footer) // 4  # Approximate token count
            content_max_length = min(len(content), (max_length - current_length - header_length - footer_length) * 4)
            
            if content_max_length < 200:  # Not enough space for meaningful content
                break
                
            # Truncate content if necessary
            if len(content) > content_max_length:
                content = content[:content_max_length] + "..."
                
            formatted_context = context_header + content + "\n" + context_footer
            formatted_contexts.append(formatted_context)
            
            # Update current length (approximate token count)
            current_length += (len(formatted_context) // 4)
            
            if current_length >= max_length:
                break
                
        return "".join(formatted_contexts)
      def build_prompt(self, user_query, contexts):
        """
        KullanÄ±cÄ± sorgusu ve baÄŸlamlardan geliÅŸmiÅŸ bir LLM prompt'u oluÅŸtur.
        
        Modern RAG teknikleri (2025):
        1. Daha yapÄ±landÄ±rÄ±lmÄ±ÅŸ baÄŸlam teslimi
        2. AÃ§Ä±kÃ§a belirtilmiÅŸ roller ve yÃ¶nergeler
        3. Kaynak alÄ±ntÄ±lama yÃ¶nergesi
        4. YanÄ±t format yapÄ±sÄ±
        5. DÃ¼ÅŸÃ¼nce zinciri (Chain-of-thought) teÅŸviki
        6. Ã–z deÄŸerlendirme talimati
        """
        # BaÄŸlamlarÄ± hazÄ±rla
        formatted_contexts = self._prepare_contexts(contexts)
        
        # Dil belirleme
        language_instruction = "TÃ¼rkÃ§e olarak"  # VarsayÄ±lan TÃ¼rkÃ§e
        if self.language != "tr":
            language_instruction = "Ä°ngilizce olarak"
            
        # GeliÅŸmiÅŸ Ã§ok aÅŸamalÄ± sorgulama yapÄ±sÄ±
        prompt = f"""Siz Baldur's Gate 3 konusunda uzmanlaÅŸmÄ±ÅŸ bir yapay zeka asistanÄ±sÄ±nÄ±z.

### SORU:
{user_query}

### BAÄLAM BÄ°LGÄ°LERÄ°:
{formatted_contexts}

### TALÄ°MATLAR:
1. Ã–nce verilen baÄŸlamlarÄ± analiz edin ve soruyla en alakalÄ± bilgileri belirleyin.
2. KullanÄ±cÄ±nÄ±n sorusunu {language_instruction} ve sadece verilen baÄŸlam bilgilerine dayanarak yanÄ±tlayÄ±n.
3. YanÄ±tÄ±nÄ±zÄ± yapÄ±landÄ±rÄ±n:
   - Ä°lk olarak, sorunun kÄ±sa ve net bir yanÄ±tÄ±nÄ± verin
   - ArdÄ±ndan, gerekirse ek baÄŸlam veya detaylar ekleyin
   - Son olarak, oyuncunun bu bilgiyi oyun iÃ§inde nasÄ±l kullanabileceÄŸi hakkÄ±nda 1-2 pratik Ã¶neri sunun

4. EÄŸer verilen bilgilerde doÄŸrudan bir cevap yoksa, baÄŸlamdaki en alakalÄ± bilgileri kullanarak bir yanÄ±t oluÅŸturun ve bilginizin sÄ±nÄ±rlÄ± olduÄŸunu dÃ¼rÃ¼stÃ§e belirtin.
5. EÄŸer sorunun yanÄ±tÄ± baÄŸlamlarda tamamen yoksa, "Bu konuda yeterli bilgiye sahip deÄŸilim" deyin ve alakalÄ± olabilecek diÄŸer bilgileri Ã¶nerin.
6. Cevaplar net, doÄŸru ve konu odaklÄ± olmalÄ±.
7. EÄŸer cevap Ä°ngilizce bir kaynaktan geliyorsa, bunu dÃ¼zgÃ¼n bir ÅŸekilde TÃ¼rkÃ§eye Ã§evirin.
8. VerdiÄŸiniz bilgilerin hangi kaynaktan geldiÄŸini aÃ§Ä±kÃ§a belirtin. Ã–rneÄŸin: "Kaynak 2'ye gÃ¶re..."
9. YanÄ±tÄ±nÄ±zÄ±n sonunda, soruyu tam olarak cevaplayÄ±p cevaplamadÄ±ÄŸÄ±nÄ±zÄ± deÄŸerlendirin ve gerekirse ek araÅŸtÄ±rma iÃ§in Ã¶neriler sunun.

### YANITINIZ:
"""
        
        return prompt
    
    def process_response_for_turkish(self, response):
        """LLM yanÄ±tÄ±nÄ± TÃ¼rkÃ§e karakterleri koruyacak ÅŸekilde iÅŸle."""
        # TÃ¼rkÃ§e karakterlerin dÃ¼zgÃ¼n gÃ¶rÃ¼ntÃ¼lenmesi iÃ§in kontroller
        tr_replacements = {
            'Ã„Â±': 'Ä±',
            'ÃƒÂ¼': 'Ã¼',
            'ÃƒÂ¶': 'Ã¶',
            'Ã…Å¸': 'ÅŸ', 
            'ÃƒÂ§': 'Ã§',
            'Ã„Å¾': 'ÄŸ',
            'Ã„Â°': 'Ä°',
            'ÃƒÅ“': 'Ãœ',
            'Ãƒâ€“': 'Ã–',
            'Ã…Å¾': 'Å',
            'Ãƒâ€¡': 'Ã‡',
            'Ã„Å¾': 'Ä'
        }
        
        for wrong, correct in tr_replacements.items():
            response = response.replace(wrong, correct)
            
        return response
    
    def ask_llm(self, prompt):
        """LLM'e prompt gÃ¶nder ve yanÄ±t al."""
        if not self.llm_client.is_available():
            logger.warning("LLM API yapÄ±landÄ±rÄ±lmamÄ±ÅŸ. Ã–neriler devre dÄ±ÅŸÄ±.")
            return "LLM API yapÄ±landÄ±rÄ±lmamÄ±ÅŸ. AyarlarÄ±nÄ±zÄ± kontrol edin."
            
        try:
            # GameState nesnesinin Ã¶zelliklerini prompt'a gÃ¶re ayarla
            from agent.decision_engine import GameState
            game_state = GameState()
            game_state.detected_keywords = prompt.split()[:5]  # Ä°lk 5 kelimeyi anahtar kelime olarak kullan
            
            # LLM'den yanÄ±t al
            recommendations = self.llm_client.get_recommendation(game_state, "general")
            
            if not recommendations:
                return "LLM'den yanÄ±t alÄ±namadÄ±."
                
            response = "\n".join(recommendations)
            # TÃ¼rkÃ§e karakter dÃ¼zeltmesi yap
            response = self.process_response_for_turkish(response)
            return response
            
        except Exception as e:
            logger.error(f"LLM yanÄ±tÄ± alÄ±nÄ±rken hata: {str(e)}")
            return f"Hata oluÅŸtu: {str(e)}"
    
    def _is_rate_limited(self):
        """Sorgu hÄ±zÄ± sÄ±nÄ±rÄ±na ulaÅŸÄ±lÄ±p ulaÅŸÄ±lmadÄ±ÄŸÄ±nÄ± kontrol et"""
        current_time = time.time()
        if current_time - self.last_query_time < self.rate_limit:
            return True
        return False
      def ask_game_ai(self, user_input):
        """
        Oyuncu sorusunu al, RAG sistemini kullanarak yanÄ±tla ve HUD'da gÃ¶ster.
        
        GeliÅŸmiÅŸ RAG akÄ±ÅŸÄ± (2025):
        1. Sorgu Ã¶n iÅŸleme ve geniÅŸletme
        2. Hibrit (semantik + anahtar kelime) arama
        3. Dinamik baÄŸlam seÃ§imi
        4. GeliÅŸmiÅŸ prompt yapÄ±landÄ±rmasÄ±
        5. Hata toleransÄ± ve yedek mekanizmalar
        6. YanÄ±t sonrasÄ± iÅŸleme
        
        Args:
            user_input: KullanÄ±cÄ±nÄ±n sorusu/girdisi
            
        Returns:
            str: LLM yanÄ±tÄ±
        """
        if not self.is_initialized:
            logger.error("RAG AsistanÄ± baÅŸlatÄ±lmadÄ±. initialize() metodunu Ã§aÄŸÄ±rÄ±n.")
            return "RAG AsistanÄ± baÅŸlatÄ±lmadÄ±."
        
        # Sorgu sÄ±klÄ±ÄŸÄ± kontrolÃ¼ yap
        if self._is_rate_limited():
            wait_time = self.rate_limit - (time.time() - self.last_query_time)
            msg = f"LÃ¼tfen {wait_time:.1f} saniye bekleyin..."
            logger.info(f"Sorgu sÄ±klÄ±ÄŸÄ± sÄ±nÄ±rÄ±na takÄ±ldÄ±: {msg}")
            self.hud_queue.put(msg)
            return msg
        
        try:
            # Sorgu zamanÄ±nÄ± gÃ¼ncelle
            self.last_query_time = time.time()
            
            # KullanÄ±cÄ± girdi bilgisini gÃ¼nlÃ¼ÄŸe kaydet
            logger.info(f"KullanÄ±cÄ± sorusu: {user_input}")
            
            # HUD'a yÃ¼kleniyor mesajÄ± gÃ¶nder
            self.hud_queue.put(f"'{user_input}' iÃ§in yanÄ±t aranÄ±yor...")
            
            # 1. Sorgu Ã¶n iÅŸleme - basit normalizasyon
            cleaned_query = user_input.strip()
            
            # 2. GeliÅŸmiÅŸ arama stratejisi - daha fazla sonuÃ§ alÄ±p sonra filtrele
            initial_k = 8  # Ä°lk aramada daha fazla sonuÃ§ al
            contexts = self.rag_search(cleaned_query, top_k=initial_k)
            
            # 3. Geri dÃ¶nÃ¼ÅŸ stratejisi - sonuÃ§ bulunamazsa
            if not contexts:
                # Daha basit anahtar kelimelerle yeniden dene
                import re
                keywords = re.findall(r'\b\w{4,}\b', cleaned_query)
                if keywords and len(keywords) >= 2:
                    fallback_query = ' '.join(keywords[:3])  # En uzun 3 kelimeyi kullan
                    logger.info(f"Ä°lk arama baÅŸarÄ±sÄ±z, ÅŸununla yeniden deneniyor: '{fallback_query}'")
                    contexts = self.rag_search(fallback_query, top_k=initial_k)
            
            # 4. Yine sonuÃ§ yoksa bilgilendirici yanÄ±t ver
            if not contexts:
                response = "Bu konu hakkÄ±nda bilgi tabanÄ±mda yeterli bilgi bulunamadÄ±. LÃ¼tfen sorunuzu farklÄ± ÅŸekilde sorun veya daha genel bir konuyla ilgili bilgi isteyin."
                self.hud_queue.put(response)
                return response
            
            # 5. En alakalÄ± olanlarÄ± seÃ§ (Ã¶rn. en iyi 5)
            best_contexts = contexts[:5]
            
            # 6. LLM promptunu oluÅŸtur
            prompt = self.build_prompt(cleaned_query, best_contexts)
            
            # 7. LLM'e gÃ¶nder ve yanÄ±tÄ± al
            response = self.ask_llm(prompt)
            
            # 8. YanÄ±t sonrasÄ± iÅŸleme - formatla ve temizle
            # Fazla boÅŸluklarÄ± temizle ve kaynak formatÄ±nÄ± dÃ¼zelt
            response = response.replace("\n\n\n", "\n\n").strip()
            response = self.process_response_for_turkish(response)
            
            # 9. HUD'da gÃ¶ster (daha dÃ¼zenli ve okunaklÄ± formatla)
            formatted_response = f"""
ğŸ“ Soru: {user_input}

ğŸ” YanÄ±t: 
{response}
            """
            self.hud_queue.put(formatted_response)
            
            # 10. Metrik kaydetme - gelecekte analiz iÃ§in
            # Burada gelecekteki iyileÅŸtirmeler iÃ§in baÅŸarÄ±lÄ± sorgu tamamlanma metriÄŸi kaydedilebilir
            
            return response
            
        except Exception as e:
            # Hata durumunda yedek yanÄ±t stratejisi
            error_msg = f"Soru yanÄ±tlanÄ±rken hata: {str(e)}"
            logger.error(error_msg)
            
            # KullanÄ±cÄ±ya daha yardÄ±mcÄ± bir hata mesajÄ± gÃ¶ster
            friendly_error = "YanÄ±t oluÅŸturulurken teknik bir sorun oluÅŸtu. LÃ¼tfen birazdan tekrar deneyin veya sorunuzu farklÄ± ÅŸekilde sorun."
            self.hud_queue.put(friendly_error)
            
            return friendly_error
    
    def shutdown(self):
        """RAG AsistanÄ±nÄ± dÃ¼zgÃ¼n ÅŸekilde kapat."""
        if self.hud:
            logger.info("HUD kapatÄ±lÄ±yor...")
            self.hud.stop()
            self.hud.join(timeout=2)
        logger.info("RAG AsistanÄ± kapatÄ±ldÄ±")


# Ã–rnek kullanÄ±m
if __name__ == "__main__":
    import time
    
    print("RAG AsistanÄ± test ediliyor...")
    assistant = RAGAssistant()
    
    if not assistant.initialize():
        print("RAG AsistanÄ± baÅŸlatÄ±lamadÄ±.")
        sys.exit(1)
        
    try:
        # BirkaÃ§ Ã¶rnek soru sor
        questions = [
            "Shadowheart kimdir?",
            "Emerald Grove'da hangi gÃ¶revler var?",
            "Baldur's Gate 3'te savaÅŸ sistemi nasÄ±l Ã§alÄ±ÅŸÄ±r?",
            "Ã‡Ä±kÄ±ÅŸ"
        ]
        
        for question in questions:
            if question.lower() == "Ã§Ä±kÄ±ÅŸ":
                break
                
            print(f"\nSoru: {question}")
            response = assistant.ask_game_ai(question)
            print(f"YanÄ±t: {response}")
            time.sleep(5)  # HUD'daki yanÄ±tÄ± gÃ¶rmek iÃ§in bekle
            
    except KeyboardInterrupt:
        print("\nKlavye kesintisi alÄ±ndÄ±.")
    finally:
        print("RAG AsistanÄ± kapatÄ±lÄ±yor...")
        assistant.shutdown()
        print("Test tamamlandÄ±.")